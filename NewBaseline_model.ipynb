{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Baseline Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What this code does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a script for a baseline, to get you started. It just calculates the mean probabilites of dangerous objects for all zones from the training samples and submits these. \n",
    "\n",
    "This approach gives 0.29 of log-loss on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jehun\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\jehun\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\jehun\\Anaconda3\\lib\\site-packages\\sklearn\\lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "C:\\Users\\jehun\\Anaconda3\\lib\\site-packages\\sklearn\\learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\jehun\\Anaconda3\\lib\\site-packages\\sklearn\\qda.py:6: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn import *\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.680101\tvalid-logloss:0.680365\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.379441\tvalid-logloss:0.391032\n",
      "[100]\ttrain-logloss:0.314986\tvalid-logloss:0.338946\n",
      "[150]\ttrain-logloss:0.296451\tvalid-logloss:0.332806\n",
      "Stopping. Best iteration:\n",
      "[147]\ttrain-logloss:0.297088\tvalid-logloss:0.332727\n",
      "\n",
      "[0]\ttrain-logloss:0.680108\tvalid-logloss:0.680391\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.37926\tvalid-logloss:0.389851\n",
      "[100]\ttrain-logloss:0.315098\tvalid-logloss:0.336328\n",
      "[150]\ttrain-logloss:0.297125\tvalid-logloss:0.329098\n",
      "[200]\ttrain-logloss:0.289033\tvalid-logloss:0.330224\n",
      "Stopping. Best iteration:\n",
      "[155]\ttrain-logloss:0.296006\tvalid-logloss:0.328996\n",
      "\n",
      "[0]\ttrain-logloss:0.680179\tvalid-logloss:0.680241\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.381487\tvalid-logloss:0.385122\n",
      "[100]\ttrain-logloss:0.317874\tvalid-logloss:0.328673\n",
      "[150]\ttrain-logloss:0.299991\tvalid-logloss:0.320396\n",
      "[200]\ttrain-logloss:0.291408\tvalid-logloss:0.321031\n",
      "Stopping. Best iteration:\n",
      "[167]\ttrain-logloss:0.296634\tvalid-logloss:0.320189\n",
      "\n",
      "[0]\ttrain-logloss:0.680244\tvalid-logloss:0.680124\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.383037\tvalid-logloss:0.379417\n",
      "[100]\ttrain-logloss:0.319715\tvalid-logloss:0.319669\n",
      "[150]\ttrain-logloss:0.302091\tvalid-logloss:0.309176\n",
      "[200]\ttrain-logloss:0.294466\tvalid-logloss:0.308121\n",
      "Stopping. Best iteration:\n",
      "[187]\ttrain-logloss:0.296105\tvalid-logloss:0.308004\n",
      "\n",
      "[0]\ttrain-logloss:0.680124\tvalid-logloss:0.680312\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.379915\tvalid-logloss:0.389467\n",
      "[100]\ttrain-logloss:0.315875\tvalid-logloss:0.335438\n",
      "[150]\ttrain-logloss:0.297893\tvalid-logloss:0.328431\n",
      "[200]\ttrain-logloss:0.289989\tvalid-logloss:0.329567\n",
      "Stopping. Best iteration:\n",
      "[158]\ttrain-logloss:0.296402\tvalid-logloss:0.328331\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('D:\\Backups\\KaggleJul2017\\TSA\\stage1_labels.csv') \n",
    "test = pd.read_csv('D:\\Backups\\KaggleJul2017\\TSA\\stage1_sample_submission.csv')\n",
    "\n",
    "trainp = train.copy()\n",
    "trainp['Id'] = trainp['Id'].map(lambda x: x.split('_')[1])\n",
    "\n",
    "piv = pd.pivot_table(trainp, columns='Id', values='Probability', aggfunc='mean', fill_value=0)\n",
    "d = pd.DataFrame.to_dict(piv)\n",
    "#from https://www.kaggle.com/philippsp/baseline-lb-0-29089\n",
    "#d['Zone9']['Probability'] = 0.05\n",
    "\n",
    "y = train['Probability'].values\n",
    "pid = test['Id'].values\n",
    "\n",
    "df_all = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "df_all['len'] = df_all['Id'].map(len)\n",
    "\n",
    "for i in range(38):\n",
    "    df_all['c'+str(i)] = df_all['Id'].map(lambda x: str(x[i]))\n",
    "df_all['c38'] = df_all['Id'].map(lambda x: str(x[i]) if len(x)==39 else '')\n",
    "df_all = df_all.drop(['Id','Probability','len'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for c in df_all.columns:\n",
    "    if df_all[c].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        df_all[c] = lbl.fit_transform(df_all[c].values)\n",
    "        #print(c, len(df_all[c].unique()))\n",
    "\n",
    "train = df_all.iloc[:len(train)]\n",
    "test = df_all.iloc[len(train):]\n",
    "\n",
    "params = {\n",
    "    'eta': 0.02,\n",
    "    'max_depth': 5,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'logloss',\n",
    "    'seed': 12,\n",
    "    'silent': True\n",
    "}\n",
    "\n",
    "fold = 5\n",
    "for i in range(fold):\n",
    "    x1, x2, y1, y2 = model_selection.train_test_split(train, y, test_size=0.18, random_state=i)\n",
    "    watchlist = [(xgb.DMatrix(x1, y1), 'train'), (xgb.DMatrix(x2, y2), 'valid')]\n",
    "    model = xgb.train(params, xgb.DMatrix(x1, y1), 1000,  watchlist, verbose_eval=50, early_stopping_rounds=50)\n",
    "    if i != 0:\n",
    "        pred += model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit)\n",
    "    else:\n",
    "        pred = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "pred /= fold\n",
    "\n",
    "submission = pd.DataFrame(pred, columns=['Probability'])\n",
    "submission['Id'] = pid\n",
    "submission['Probability'] += submission['Id'].map(lambda x: d[str(x).split('_')[1]]['Probability'])*3\n",
    "submission['Probability'] /= 4\n",
    "submission.to_csv('submission_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, the author used his psychological knowledge to predict that images with dangerous objects in Zone9 are underrepresented in the test set (you can guess why when you look at where the zone is ;-)), and accordingly reduce their probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem with this approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major problem of this approach is that it ignores entirely the contents of the .aps files (or any other formats). \n",
    "\n",
    "The training and test sets, under the above approach, consists of the encoded bits of the image label, such as `00360f79fd6e02781457eda48f85da90_Zone1`. This is obviously not a valid approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
